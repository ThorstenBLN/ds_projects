{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklego.preprocessing import RepeatingBasisFunction\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data in split into X & Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('../data/train.csv', parse_dates=['datetime'])\n",
    "# split into x and y data\n",
    "X = df.drop(['count', 'casual', 'registered'], axis=1)\n",
    "y = df['count']\n",
    "# use logarithm(y + 1) transformation on y \n",
    "y_log = np.log1p(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create pipeline, columntransformer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the needed time features\n",
    "def date_time_transformation(df):\n",
    "    X = df\n",
    "    X['hour'] = X['datetime'].dt.hour\n",
    "    X['dayofyear'] = X['datetime'].dt.dayofyear\n",
    "    X['year'] = X['datetime'].dt.year\n",
    "    X = X.drop('datetime', axis=1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min max scale and Polynomial for numerical features (NOT USED)\n",
    "numerical_features = ['atemp', 'humidity']\n",
    "numerical_transformer = make_pipeline(MinMaxScaler(), PolynomialFeatures(include_bias=False, interaction_only=False, degree=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the transformer preprocessor\n",
    "# use the optimized parameters\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"hour\", RepeatingBasisFunction(n_periods=14, column=\"hour\", input_range=(0,23), remainder=\"drop\"), ['hour']),\n",
    "        (\"month\", RepeatingBasisFunction(n_periods=36,column=\"dayofyear\",input_range=(1,365),remainder=\"drop\"), ['dayofyear']),\n",
    "        ('numeric_polinomial', MinMaxScaler(), ['atemp', 'humidity']),\n",
    "        ('categorical', OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore'), ['workingday', 'year']),\n",
    "    ],\n",
    "    remainder='drop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model pipeline\n",
    "# first create the time features, then transform the features with the column transformer, \n",
    "# then create polynomial features and finally feed all to the model\n",
    "# use the optimized parameters\n",
    "pipeline = make_pipeline(FunctionTransformer(date_time_transformation), \n",
    "                        preprocessor, \n",
    "                        PolynomialFeatures(include_bias=False, interaction_only=False, degree=2), \n",
    "                        Ridge(alpha=7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fit the model with Xtrain and ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to train and test\n",
    "Xtrain, Xval, ytrain_log, yval_log = train_test_split(X, y_log, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9422971471882301, 0.9411374571748645)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the pipeline/model and calculate the R2 score\n",
    "pipeline.fit(Xtrain, ytrain_log)\n",
    "pipeline.score(Xtrain, ytrain_log), pipeline.score(Xval, yval_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. calculate the MSLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the logaritmic y \n",
    "y_pred_train_log = pipeline.predict(Xtrain)\n",
    "y_pred_val_log = pipeline.predict(Xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de-log the y predictions\n",
    "y_pred_train = np.exp(y_pred_train_log) - 1\n",
    "y_pred_val = np.exp(y_pred_val_log) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1156175239380077 0.12116885048182596\n"
     ]
    }
   ],
   "source": [
    "# calculate the MSLE for train and validation data\n",
    "msle_ridge_train = mean_squared_log_error(np.expm1(ytrain_log), y_pred_train)\n",
    "msle_ridge_val = mean_squared_log_error(np.expm1(yval_log), y_pred_val)\n",
    "print(msle_ridge_train, msle_ridge_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Kaggle upload file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9430448121405939"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model with full data\n",
    "pipeline.fit(X, y_log)\n",
    "pipeline.score(X, y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import test-dataset\n",
    "Xtest = pd.read_csv('../data/test.csv', parse_dates=['datetime'])\n",
    "df_datetime = Xtest[['datetime']]\n",
    "# make logarithmic predictions\n",
    "y_pred_test_log = pipeline.predict(Xtest)\n",
    "# convert log prediction to prediction\n",
    "y_pred_test = np.exp(y_pred_test_log) - 1\n",
    "df_y_pred_test = pd.DataFrame(y_pred_test)\n",
    "# merge X datetime with y\n",
    "test_result = pd.merge(df_datetime, df_y_pred_test, left_index=True, right_index=True)\n",
    "test_result.columns = ['datetime', 'count']\n",
    "# create upload pdf\n",
    "test_result.to_csv('bike_result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d01013faa6268d9b541af39e21e51ae91e81e71b63bd76f73433c553979f7595"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
